{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Project 13: Metaphor detection in poetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Danila Goncharenko, 2303788\n",
    "\n",
    "Ana Ferreira, 2308587\n",
    "\n",
    "Mikhail Bichagov, 2304806"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This project explores the detection of metaphors in poetry using natural language processing, aiming to distinguish figurative and non-figurative language. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall consider the common use of a phrase as literal use and its violation as an indicative of metaphorical use. The project initially attempts to imitate the approach of Neuman et al. (2013) published in PlusOne journal -Metaphor Identification in Large Texts Corpora- available online [`Metaphor Identification in Large Texts Corpora (plos.org)`](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0062343). So first consider the British national corpus (BNCCorpus), available through NLTK (see also [`British National Corpus, XML edition (ox.ac.uk)`](https://ota.bodleian.ox.ac.uk/repository/xmlui/handle/20.500.12024/2554)). For testing, we shall consider the annotated corpus available at https://www.eecs.uottawa.ca/~diana/resources/metaphor/type1_metaphor_annotated.txt \n",
    "\n",
    "In the above, the annotation at the end of the sentence i.e., @1@y   indicates whether it is a metaphor (y) or not (n). Here the presence of ‘y’ indicates that it is a metaphor, whereas “1” indicates the first head word of the sentence, which is “poise”, in the part of speech tag sequence. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords, CategorizedPlaintextCorpusReader\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import FreqDist, bigrams\n",
    "from itertools import chain\n",
    "\n",
    "from nltk.collocations import *\n",
    "\n",
    "# Downloading the BNC corpus and stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Stopwords, Lemmatizer, Bi-gram\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.bnc import BNCCorpusReader\n",
    "\n",
    "# CHANGE THE PATH\n",
    "bnc_reader = BNCCorpusReader(root=\"C:/Users/Dan/Desktop/NLP Project/BNC/Texts\", fileids=r'[A-K]/\\w*/\\w*\\.xml')\n",
    "\n",
    "# list_of_fileids = ['A/A0/A00.xml', 'A/A0/A01.xml']\n",
    "# finder = BigramCollocationFinder.from_words(bnc_reader.words(fileids=list_of_fileids))\n",
    "# scored = finder.score_ngrams(bigram_measures.raw_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we shall consider the mutual information, see expression (2) in Neuman et al.’2003 paper, as a guideline to derive the metaphor-reasoning.  You can inspire from other available implementations of mutual information, in [`Collocations (nltk.org)`](https://www.nltk.org/howto/collocations.html), [`FNLP 2011: Tutorial 8: Working with corpora: mutual information (ed.ac.uk)`](http://www.inf.ed.ac.uk/teaching/courses/fnlp/lectures/8/tutorial.html). Consider the words “woman”, “use”, “dream”, “body”. Write a program that identifies all adjectives, adverbs and verbs that occur within 2 lexical units (span = 2 in the formula of mutual information) in BNC corpus and whose mutual information is equal or greater than 3, considered as the minimum statistical significance. Suggest appropriate adjustments (e.g., greater span) if no results are found to match the mutual information criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BNC Baby Corpus, diff formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('woman', 'receiving'): 18.043012116810814,\n",
       " ('illness', 'woman'): 17.043012116810814,\n",
       " ('multiplies', 'body'): 16.72108402192345,\n",
       " ('body', 'seriously'): 16.72108402192345,\n",
       " ('damage', 'body'): 16.72108402192345,\n",
       " ('body', 'kill'): 16.72108402192345,\n",
       " ('enters', 'body'): 15.721084021923451,\n",
       " ('body', 'enough'): 15.721084021923451,\n",
       " ('weakened', 'body'): 15.721084021923451,\n",
       " ('words', 'use'): 15.043012116810814,\n",
       " ('cells', 'body'): 14.721084021923451,\n",
       " ('body', 'much'): 14.721084021923451,\n",
       " ('use', 'condom'): 14.458049616089657,\n",
       " ('use', 'legacy'): 14.458049616089657,\n",
       " ('always', 'use'): 13.458049616089657,\n",
       " ('value', 'use'): 13.458049616089657,\n",
       " ('body', 'get'): 13.261652403286153,\n",
       " ('leaflet', 'use'): 13.043012116810813,\n",
       " ('could', 'use'): 12.34257239866972,\n",
       " ('use', 'drugs'): 11.8730871153685,\n",
       " ('use', 'deed'): 11.72108402192345,\n",
       " ('income', 'use'): 11.5194501607538,\n",
       " ('payment', 'use'): 11.185031121683242,\n",
       " ('people', 'use'): 11.136121521202295,\n",
       " ('use', 'gift'): 11.065732193310897,\n",
       " ('use', 'make'): 10.685460112192729,\n",
       " ('use', 'covenant'): 9.955549275560474}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus.reader.bnc import BNCCorpusReader\n",
    "from nltk import FreqDist, bigrams\n",
    "from itertools import chain\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import *\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# Replace 'path_to_bnc_data' with the actual path to your downloaded BNC XML data\n",
    "path_to_bnc_data = 'C:/Users/jklbichami/OneDrive - Valmet/Documents/School/Porgramming/NLP/bnc/Texts/news/'\n",
    "\n",
    "# Initialize the BNC corpus reader\n",
    "bnc_reader = BNCCorpusReader(root=path_to_bnc_data, fileids=r'[A-K]//w*//w*/.xml')\n",
    "\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "# Here need to merge all of the words from different xlm files together.\n",
    "# Its just an exmaple to show what it could look\n",
    "\n",
    "corpus = bnc_reader.words('A1E.xml')\n",
    "\n",
    "words_to_include = ['woman', 'use', 'dream', 'body']\n",
    "\n",
    "# Get the words from the file in list format\n",
    "es = stopwords.words('english')\n",
    "\n",
    "g = FreqDist(bigrams(w.lower() for w in corpus if (w.isalpha() and w.lower() not in es)))\n",
    "\n",
    "f = FreqDist()\n",
    "for k in g.keys():\n",
    "    if k[1] in words_to_include or k[0] in words_to_include:\n",
    "        f[k] = g.get(k)\n",
    "\n",
    "# Here we probably should not limit words by anything, except just removing stop words\n",
    "u = FreqDist(w.lower() for w in corpus if (w.isalpha() and not(w.lower() in es)))\n",
    "\n",
    "## If we have double nested list then we can use this to flatten it\n",
    "chain.from_iterable(corpus)\n",
    "\n",
    "from math import log\n",
    "def mutInf(p,u1,u2,b):\n",
    "    try:\n",
    "        return log((float(b[p])/float(b.N()))/\n",
    "                   ((float(u1[p[0]])*float(u2[p[1]]))/\n",
    "                    (float(u1.N())*float(u2.N()))),\n",
    "                   2)\n",
    "    except:\n",
    "        return\n",
    "\n",
    "fmi = {}\n",
    "for p in f.keys():\n",
    "    fmi[p]  = mutInf(p,u,u,f)\n",
    "\n",
    "fmi = {key: value for key, value in fmi.items() if value > 3}\n",
    "\n",
    "dict(sorted(fmi.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BNC full Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expression (2) in Neuman et al.’2013 paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See expression (2) in Neuman et al.’2013 paper\n",
    "import math\n",
    "def Mutual_information(bigram_item, filteredCorpus, Corpus, span = 2):    \n",
    "    '''\n",
    "    Calculates Mutual information between node and collocate words\n",
    "    bigram_item = The bigram which is considered in the equation\n",
    "    filteredCorpus = The corpus with only considered words.\n",
    "    Corpus = The whole corpus.\n",
    "    span = span of words\n",
    "    '''\n",
    "\n",
    "    # filteredCorpus[fr_B_near_A] = frequency of collocate near the node word (e.g., color near purple)\n",
    "    # Corpus.N() = size of the corpus (for instance 96,263,399, BNC)\n",
    "    # Corpus[p[0]] = frequency of node word w1 (e.g., purple): 1262\n",
    "    # Corpus[p[1]] = frequency of collocate word w2 (e.g., color): 115\n",
    "    # span = span of words (e.g., 1 to left and 1 to right of the node word: 2)\n",
    "\n",
    "    #sizeCorpus = 96 132 981 tokens in BNC\n",
    "\n",
    "    try:\n",
    "        return math.log10((filteredCorpus[bigram_item] * Corpus.N()) / (Corpus[p[0]] * Corpus[p[1]] * span))/math.log10(2)\n",
    "    except:\n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequencies calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = bnc_reader.words('A1E.xml')\n",
    "corpus = bnc_reader.words('A/A0/A01.xml')\n",
    "\n",
    "# Consider the words “woman”, “use”, “dream”, “body”.\n",
    "words_to_include = ['woman', 'use', 'dream', 'body']\n",
    "\n",
    "# Calculate the frequency of bigrams of all words in the corpus\n",
    "bigram_frequency = FreqDist(bigrams(w.lower() for w in corpus if (w.isalpha() and w.lower() not in stop_words)))\n",
    "\n",
    "# Calculate the frequency of bigrams of the words to include that are in the corpus\n",
    "fr_words_to_include = FreqDist()\n",
    "for key_word in bigram_frequency.keys():\n",
    "    # If first or second word in bigram, e.g. ('woman', 'receiving'), has a word_to_include\n",
    "    # Pass the frequency of this bigram from bigram_frequency to fr_words_to_include\n",
    "    if key_word[1] in words_to_include or key_word[0] in words_to_include:\n",
    "        fr_words_to_include[key_word] = bigram_frequency.get(key_word)\n",
    "\n",
    "# Frequency of all words in the corpus, except stop words\n",
    "unigram_frequency = FreqDist(w.lower() for w in corpus if (w.isalpha() and not(w.lower() in stop_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Mutual information frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('woman', 'receiving'): 9.925183519434208,\n",
       " ('illness', 'woman'): 8.925183519434208,\n",
       " ('multiplies', 'body'): 8.603255424546846,\n",
       " ('body', 'seriously'): 8.603255424546846,\n",
       " ('damage', 'body'): 8.603255424546846,\n",
       " ('body', 'kill'): 8.603255424546846,\n",
       " ('enters', 'body'): 7.603255424546846,\n",
       " ('body', 'enough'): 7.603255424546846,\n",
       " ('weakened', 'body'): 7.603255424546846,\n",
       " ('words', 'use'): 6.925183519434207,\n",
       " ('cells', 'body'): 6.6032554245468456,\n",
       " ('body', 'much'): 6.6032554245468456,\n",
       " ('use', 'condom'): 6.340221018713052,\n",
       " ('use', 'legacy'): 6.340221018713052,\n",
       " ('always', 'use'): 5.340221018713052,\n",
       " ('value', 'use'): 5.340221018713052,\n",
       " ('body', 'get'): 5.143823805909549,\n",
       " ('leaflet', 'use'): 4.925183519434208,\n",
       " ('could', 'use'): 4.2247438012931156,\n",
       " ('use', 'drugs'): 3.7552585179918956,\n",
       " ('use', 'deed'): 3.603255424546846,\n",
       " ('income', 'use'): 3.401621563377195,\n",
       " ('payment', 'use'): 3.0672025243066363,\n",
       " ('people', 'use'): 3.0182929238256895}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## If we have double nested list then we can use this to flatten it\n",
    "chain.from_iterable(corpus)\n",
    "\n",
    "# Identify all adjectives, adverbs and verbs that occur within 2 lexical units (span = 2 in the formula of mutual information) in BNC corpus\n",
    "# Mutual information frequency\n",
    "fmi = {}\n",
    "for p in fr_words_to_include.keys():\n",
    "    fmi[p]  = Mutual_information(p, fr_words_to_include, unigram_frequency)\n",
    "\n",
    "# Mutual information is equal or greater than 3, considered as the minimum statistical significance\n",
    "fmi = {key: value for key, value in fmi.items() if value > 3}\n",
    "\n",
    "dict(sorted(fmi.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggest appropriate adjustments (e.g., greater span), if no results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suggest appropriate adjustments (e.g., greater span), if no results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to test this process in the previous metaphor annotated dataset. For this purpose, consider the following approach. Write a program that inputs each sentence of the annotated corpus, and then reads the head word (given in the annotation), then calculate the mutual distance between the head-word and each of the first two words occurring either on the left hand side part or right hand side part of the head-word. If the average of mutual distances from head word to each of the two words situated at two lexical units is greater than 3, then we shall consider the sentence is not a metaphor, otherwise, it is a metaphor. Test this reasoning and report the result for each annotated sentence and save it in your database. Given the ground truth of the annotated dataset, calculate the corresponding accuracy, and comment on the efficiency of the proposed approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Number</th>\n",
       "      <th>Head-word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>poise is a club .</td>\n",
       "      <td>y</td>\n",
       "      <td>1</td>\n",
       "      <td>poise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>destroying alexandria . sunlight is silence</td>\n",
       "      <td>y</td>\n",
       "      <td>4</td>\n",
       "      <td>sunlight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>feet are no anchor . gravity sucks at the mind</td>\n",
       "      <td>y</td>\n",
       "      <td>1</td>\n",
       "      <td>feet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>on the day 's horizon is a gesture of earth</td>\n",
       "      <td>y</td>\n",
       "      <td>5</td>\n",
       "      <td>horizon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>he said good-by as if good-by is a number .</td>\n",
       "      <td>y</td>\n",
       "      <td>6</td>\n",
       "      <td>good-by</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>as the season of cold is the season of darkness</td>\n",
       "      <td>n</td>\n",
       "      <td>5</td>\n",
       "      <td>cold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>else all beasts were tigers ,</td>\n",
       "      <td>y</td>\n",
       "      <td>3</td>\n",
       "      <td>beasts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>without which earth is sand</td>\n",
       "      <td>n</td>\n",
       "      <td>3</td>\n",
       "      <td>earth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>the sky is cloud on cloud</td>\n",
       "      <td>n</td>\n",
       "      <td>2</td>\n",
       "      <td>sky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>the sky is cloudy</td>\n",
       "      <td>n</td>\n",
       "      <td>2</td>\n",
       "      <td>sky</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>619 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Text Symbol  Number Head-word\n",
       "0                                  poise is a club .       y       1     poise\n",
       "1        destroying alexandria . sunlight is silence       y       4  sunlight\n",
       "2     feet are no anchor . gravity sucks at the mind       y       1      feet\n",
       "3        on the day 's horizon is a gesture of earth       y       5   horizon\n",
       "4        he said good-by as if good-by is a number .       y       6   good-by\n",
       "..                                                ...    ...     ...       ...\n",
       "614  as the season of cold is the season of darkness       n       5      cold\n",
       "615                    else all beasts were tigers ,       y       3    beasts\n",
       "616                      without which earth is sand       n       3     earth\n",
       "617                        the sky is cloud on cloud       n       2       sky\n",
       "618                                the sky is cloudy       n       2       sky\n",
       "\n",
       "[619 rows x 4 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://www.eecs.uottawa.ca/~diana/resources/metaphor/type1_metaphor_annotated.txt\"\n",
    "\n",
    "# Read the data into a DataFrame\n",
    "df = pd.read_csv(url, delimiter='\\t', header=None, names=['Text'])\n",
    "\n",
    "# Extract the symbol ('y' or 'n') and number into separate columns\n",
    "df['Symbol'] = df['Text'].str.extract(r'@(\\d+)@([yn])')[1]\n",
    "df['Number'] = df['Text'].str.extract(r'@(\\d+)@([yn])')[0]\n",
    "\n",
    "# Delete it from the original text\n",
    "df['Text'] = df['Text'].str.replace(r'(@\\d+@y|@\\d+@n)', '', regex=True)\n",
    "\n",
    "# Replace NaN values in 'Number' with 0, and then convert to integer\n",
    "df['Number'] = df['Number'].fillna(0).astype(int)\n",
    "\n",
    "# Extract the word with the specified index and save it in the 'Head-word' column\n",
    "df['Head-word'] = df.apply(lambda row: row['Text'].split()[int(row['Number']) - 1], axis=1)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input each sentence of the annotated corpus\n",
    "\n",
    "# Read the head word (given in the annotation)\n",
    "\n",
    "# Calculate the mutual distance between the head-word and each of the first two words\n",
    "# either on the left hand side part or right hand side part of the head-word.\n",
    "\n",
    "# If average is greater than 3, sentence is not a metaphor\n",
    "\n",
    "# Report the result for each annotated sentence \n",
    "\n",
    "# Save it in your database\n",
    "\n",
    "# Calculate the corresponding accuracy\n",
    "\n",
    "# Comment on the efficiency of the proposed approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the (adjective-noun) type of metaphor (referred to as Metaphor type III). A metaphor  assumes to occur when the categories of noun and adjective are such that one is concrete and the other one is abstract. WordStat noun categorization based on WordNet, which classifies 69,817 nouns into 25 categories, of which 13 are concrete categories (e.g., artifact) provides a database for a such categorization. It is freely available in [`Wordnet based categorization dictionary - Provalis Research`](https://provalisresearch.com/products/content-analysis-software/wordstat-dictionary/wordnet-based-categorization-dictionary/). Write a program that allows you to retrieve the category of noun and adjective / adverb in a sentence according to WordStat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjective-noun type of metaphor is a Metaphor type III.\n",
    "\n",
    "# If categories of noun and adjective: one is concrete and the other one is abstract,\n",
    "# Then it is a Metaphor.\n",
    "\n",
    "# WordStat noun categorization based on WordNet provides a database for a such categorization.\n",
    "\n",
    "# Retrieve the category of noun and adjective / adverb in a sentence according to WordStat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jklbichami\\OneDrive - Valmet\\Documents\\School\\Porgramming\\NLP\\Metaphor-detection-in-poetry\\Project NLP.ipynb Cell 27\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jklbichami/OneDrive%20-%20Valmet/Documents/School/Porgramming/NLP/Metaphor-detection-in-poetry/Project%20NLP.ipynb#Y226sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m noun_categories \u001b[39m=\u001b[39m [a\u001b[39m.\u001b[39mstrip(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)[\u001b[39m6\u001b[39m:] \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m cat_data \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m a \u001b[39mand\u001b[39;00m a\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mNOUN.\u001b[39m\u001b[39m'\u001b[39m)]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jklbichami/OneDrive%20-%20Valmet/Documents/School/Porgramming/NLP/Metaphor-detection-in-poetry/Project%20NLP.ipynb#Y226sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m noun_categories_idices \u001b[39m=\u001b[39m [cat_data\u001b[39m.\u001b[39mindex(\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39mNOUN.\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m a \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m noun_categories]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jklbichami/OneDrive%20-%20Valmet/Documents/School/Porgramming/NLP/Metaphor-detection-in-poetry/Project%20NLP.ipynb#Y226sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m cat_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mType\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCategory\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mWord\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jklbichami/OneDrive%20-%20Valmet/Documents/School/Porgramming/NLP/Metaphor-detection-in-poetry/Project%20NLP.ipynb#Y226sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m## Mikhail: did not finish this but idea is pretty simple\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jklbichami/OneDrive%20-%20Valmet/Documents/School/Porgramming/NLP/Metaphor-detection-in-poetry/Project%20NLP.ipynb#Y226sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m## 1. We get the indeces of all topics\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jklbichami/OneDrive%20-%20Valmet/Documents/School/Porgramming/NLP/Metaphor-detection-in-poetry/Project%20NLP.ipynb#Y226sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m## 2. We get indeces for respective categories\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jklbichami/OneDrive%20-%20Valmet/Documents/School/Porgramming/NLP/Metaphor-detection-in-poetry/Project%20NLP.ipynb#Y226sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m## 5. Assign all results to df based on indices\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jklbichami/OneDrive%20-%20Valmet/Documents/School/Porgramming/NLP/Metaphor-detection-in-poetry/Project%20NLP.ipynb#Y226sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m## Here is examplee for Nouns\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jklbichami/OneDrive%20-%20Valmet/Documents/School/Porgramming/NLP/Metaphor-detection-in-poetry/Project%20NLP.ipynb#Y226sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mfor\u001b[39;00m indx, num \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(noun_categories_idices):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "import json\n",
    "file_p = r'C:\\Users\\jklbichami\\OneDrive - Valmet\\Documents\\School\\Porgramming\\NLP\\WordNet2\\WordNet Words & Phrases.CAT'\n",
    "\n",
    "with open(file_p, 'r') as file:\n",
    "    cat_data = file.readlines()\n",
    "\n",
    "topics = [a.strip('\\n') for a in cat_data if '\\t' not in a ]\n",
    "topics_indices = [cat_data.index(a + '\\n') for a in topics]\n",
    "\n",
    "noun_categories = [a.strip('\\n')[6:] for a in cat_data if '\\t\\t' not in a and a.startswith('\\tNOUN.')]\n",
    "noun_categories_idices = [cat_data.index('\\tNOUN.' + a + '\\n') for a in noun_categories]\n",
    "\n",
    "cat_df = pd.DataFrame(columns=['Type', 'Category', 'Word'])\n",
    "\n",
    "\n",
    "## Mikhail: did not finish this but idea is pretty simple\n",
    "## 1. We get the indeces of all topics\n",
    "## 2. We get indeces for respective categories\n",
    "## 3. We iterate through each topics\n",
    "## 4. We iterate through their categories\n",
    "## 5. Assign all results to df based on indices\n",
    "## Here is examplee for Nouns\n",
    "for indx, num in enumerate(noun_categories_idices):\n",
    "    temp_dict = {}\n",
    "    if indx == 0:\n",
    "        temp_dict['Category'] = [noun_categories[indx]]*(26600-cat_data.index('ADVERBS\\n')-2)\n",
    "        ## As many times as diff between index of Verb and Noun and -2\n",
    "        temp_dict['Type'] = ['Noun']*(26600-cat_data.index('ADVERBS\\n')-2)\n",
    "        temp_dict['Word'] = cat_data[cat_data.index('ADVERBS\\n')+1:26600-1]\n",
    "        cat_df = pd.concat([cat_df, pd.DataFrame(temp_dict)], ignore_index=True)\n",
    "    else:\n",
    "        cat_df.loc[noun_categories_idices[indx-1]:num, 'Catergory'] = noun_categories[indx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would like to imitate the procedure mentioned in Neuman’s paper for type III semaphore. Write a program that identifies the occurrence of Noun-Adjective/Adverb part-of-speech in a given sentence. Then, use WordNet lexical database to find out the number of senses of each adjective. If every adjective has one single sense, then return, no metaphor. If the Noun has no entry in wordnet, then return UNKNOWN. Otherwise (adjective has more than one sense and noun has an entry in WordNet), then identify the set S of nouns in the BNC corpus that collocate with the given Noun of the given sentence (this corresponds to a set of nouns whose mutual information value is greater or equal than 3). Next, for each element (noun) of S, use the WordStat categorization to identify those who belong to concrete class. Let S1 be a subset of S, which contains these “concrete”-category nouns. If the number of elements in S1 is large, then restrict to the first three elements who have the highest mutual information values. Finally, to find out whether, whether the sentence containing adjective A and noun N is a metaphor, we need to test the compatibility of each elements of S1 with N. If there is no elements in S1 compatible with N, then we shall consider S as a metaphor, otherwise, it is not. To evaluate this compatibility, you can use the Wu and Palmer WordNet semantic similarity already implemented in NLTK. Therefore, assume that if the Wu and Palmer semantic similarity of at least of the nouns in S1 with N is greater than a threshold 0.4, then the compatibility between S1 and N is granted. (Note this is only a very rough approximation). Write a code that implements this reasoning and test it on two simple examples of your choice. Test this process for other values of threshold values (e.g., 0.3, 0.5, 0.6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Imitate the procedure mentioned in Neuman’s paper for type III semaphore\n",
    "\n",
    "# Identify the occurrence of Noun-Adjective/Adverb part-of-speech in a given sentence.\n",
    "\n",
    "# Use WordNet lexical database to find out the number of senses of each adjective.\n",
    "\n",
    "# If every adjective has one single sense, then no metaphor\n",
    "\n",
    "# If the Noun has no entry in wordnet, then return UNKNOWN.\n",
    "\n",
    "# Otherwise: adjective has more than one sense and noun has an entry in WordNet\n",
    "\n",
    "# Identify the set S of nouns in the BNC corpus that collocate with the given Noun of the given sentence\n",
    "# set of nouns: mutual information value is >= 3\n",
    "\n",
    "# Use the WordStat categorization for each noun in S to identify those who belong to concrete class.\n",
    "\n",
    "# Let S1 be a subset of S, which contains these “concrete”-category nouns.\n",
    "# If the number of elements in S1 is large, then restrict to the first three elements who have the highest mutual information values.\n",
    "\n",
    "# Test the compatibility of each elements of S1 with N.\n",
    "# To check if the sentence containing adjective A and noun N is a metaphor\n",
    "\n",
    "# If there is no elements in S1 compatible with N, then we shall consider S as a metaphor\n",
    "\n",
    "# Evaluate, using the Wu and Palmer WordNet semantic similarity from NLTK.\n",
    "\n",
    "# if semantic similarity of at least of the nouns in S1 with N >= 0.4, then the compatibility between S1 and N is granted.\n",
    "\n",
    "# Test it on two simple examples of your choice. \n",
    "\n",
    "# Test this process for other values of threshold values (e.g., 0.3, 0.5, 0.6) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the above reasoning on the first half of the dataset https://www.eecs.uottawa.ca/~diana/resources/metaphor/type1_metaphor_annotated.txt where adjective-noun type of relationship occurs. Motivate your reasoning and answers. Estimate the accuracy accordingly, and report individual results in your database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the above reasoning on the first half of the dataset\n",
    "\n",
    "# Motivate your reasoning and answers.\n",
    "\n",
    "# Estimate the accuracy accordingly, and report individual results in your database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of the calculus of the semantic similarity between N and each elements of S1 in step 4, we would like to use the wordnet domain of each individual words. For this purpose, download the wordnet domain from [`WordNet Domains (fbk.eu)`](https://wndomains.fbk.eu/download.html). Therefore, the compatibility between N and an element N1 of S1 is granted if N and N1 belong to the same wordnet domain. Write a program that allows you to implement this reasoning and test it on simple sentences of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the wordnet domain of each individual words, instead of similarity between N and each elements of S1\n",
    "\n",
    "# Download the wordnet domain\n",
    "\n",
    "# If N and N1 belong to the same wordnet domain, compatibility between N and an element N1 of S1 is granted\n",
    "\n",
    "# Test it on simple sentences of your choice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the reasoning of 6) on the same subset of annotated metaphor dataset used in 5) and compare the performance in terms of accuracy. Save individual results in your database as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the reasoning of 6) on dataset used in 5)\n",
    "\n",
    "# Compare the performance in terms of accuracy\n",
    "\n",
    "# Save individual results in your database as well\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat 6) and 7) when using Reuter corpus (also accessible via NLTK) instead of BNC corpus. Conclude on the impact of the corpus on the accuracy of metaphor identification.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat 6) and 7) when using Reuter corpus from NLTK\n",
    "\n",
    "# Conclude on the impact of the corpus on the accuracy of metaphor identification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to enhance the reasoning of any of project specifications above, feel free to suggest any state-of-the-art approach that you judge relevant and accommodate to achieve the goal accordingly. Motivate your choice by concise literature review. Use appropriate literature to discuss your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhance the reasoning of any of project specifications above\n",
    "\n",
    "# Suggest any relevant state-of-the-art approach\n",
    "\n",
    "# Motivate your choice by concise literature review. \n",
    "\n",
    "# Use appropriate literature to discuss your findings.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
