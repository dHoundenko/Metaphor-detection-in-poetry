{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Project 13: Metaphor detection in poetry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Danila Goncharenko, 2303788\n",
    "\n",
    "Ana Ferreira, 2308587\n",
    "\n",
    "Mikhail Bichagov, 2304806"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This project explores the detection of metaphors in poetry using natural language processing, aiming to distinguish figurative and non-figurative language. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall consider the common use of a phrase as literal use and its violation as an indicative of metaphorical use. The project initially attempts to imitate the approach of Neuman et al. (2013) published in PlusOne journal -Metaphor Identification in Large Texts Corpora- available online [`Metaphor Identification in Large Texts Corpora (plos.org)`](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0062343). So first consider the British national corpus (BNCCorpus), available through NLTK (see also [`British National Corpus, XML edition (ox.ac.uk)`](https://ota.bodleian.ox.ac.uk/repository/xmlui/handle/20.500.12024/2554)). For testing, we shall consider the annotated corpus available at https://www.eecs.uottawa.ca/~diana/resources/metaphor/type1_metaphor_annotated.txt \n",
    "\n",
    "In the above, the annotation at the end of the sentence i.e., @1@y   indicates whether it is a metaphor (y) or not (n). Here the presence of ‘y’ indicates that it is a metaphor, whereas “1” indicates the first head word of the sentence, which is “poise”, in the part of speech tag sequence. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords, CategorizedPlaintextCorpusReader\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import FreqDist, bigrams\n",
    "from itertools import chain\n",
    "\n",
    "from nltk.collocations import *\n",
    "\n",
    "# Downloading the BNC corpus and stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Stopwords, Lemmatizer, Bi-gram\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.bnc import BNCCorpusReader\n",
    "\n",
    "# CHANGE THE PATH\n",
    "bnc_reader = BNCCorpusReader(root=\"C:/Users/Dan/Desktop/NLP Project/BNC/Texts\", fileids=r'[A-K]/\\w*/\\w*\\.xml')\n",
    "\n",
    "# list_of_fileids = ['A/A0/A00.xml', 'A/A0/A01.xml']\n",
    "# finder = BigramCollocationFinder.from_words(bnc_reader.words(fileids=list_of_fileids))\n",
    "# scored = finder.score_ngrams(bigram_measures.raw_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we shall consider the mutual information, see expression (2) in Neuman et al.’2003 paper, as a guideline to derive the metaphor-reasoning.  You can inspire from other available implementations of mutual information, in [`Collocations (nltk.org)`](https://www.nltk.org/howto/collocations.html), [`FNLP 2011: Tutorial 8: Working with corpora: mutual information (ed.ac.uk)`](http://www.inf.ed.ac.uk/teaching/courses/fnlp/lectures/8/tutorial.html). Consider the words “woman”, “use”, “dream”, “body”. Write a program that identifies all adjectives, adverbs and verbs that occur within 2 lexical units (span = 2 in the formula of mutual information) in BNC corpus and whose mutual information is equal or greater than 3, considered as the minimum statistical significance. Suggest appropriate adjustments (e.g., greater span) if no results are found to match the mutual information criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BNC Baby Corpus, diff formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jklbichami\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('body', 'iosco'): 21.734829502493657,\n",
       " ('body', 'middlemen'): 20.734829502493657,\n",
       " ('regulators', 'body'): 20.734829502493657,\n",
       " ('regulatory', 'body'): 20.1498670017725}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus.reader.bnc import BNCCorpusReader\n",
    "from nltk import FreqDist, bigrams\n",
    "from itertools import chain\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import *\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# Replace 'path_to_bnc_data' with the actual path to your downloaded BNC XML data\n",
    "path_to_bnc_data = 'C:/Users/jklbichami/OneDrive - Valmet/Documents/School/Porgramming/NLP/bnc/Texts/news/'\n",
    "\n",
    "# Initialize the BNC corpus reader\n",
    "bnc_reader = BNCCorpusReader(root=path_to_bnc_data, fileids=r'[A-K]//w*//w*/.xml')\n",
    "\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "# Here need to merge all of the words from different xlm files together.\n",
    "# Its just an exmaple to show what it could look\n",
    "\n",
    "corpus = bnc_reader.words('A1E.xml')\n",
    "\n",
    "words_to_include = ['woman', 'use', 'dream', 'body']\n",
    "\n",
    "# Get the words from the file in list format\n",
    "es = stopwords.words('english')\n",
    "\n",
    "g = FreqDist(bigrams(w.lower() for w in corpus if (w.isalpha() and w.lower() not in es)))\n",
    "\n",
    "f = FreqDist()\n",
    "for k in g.keys():\n",
    "    if k[1] in words_to_include or k[0] in words_to_include:\n",
    "        f[k] = g.get(k)\n",
    "\n",
    "# Here we probably should not limit words by anything, except just removing stop words\n",
    "u = FreqDist(w.lower() for w in corpus if (w.isalpha() and not(w.lower() in es)))\n",
    "\n",
    "## If we have double nested list then we can use this to flatten it\n",
    "chain.from_iterable(corpus)\n",
    "\n",
    "from math import log\n",
    "def mutInf(p,u1,u2,b):\n",
    "    try:\n",
    "        return log((float(b[p])/float(b.N()))/\n",
    "                   ((float(u1[p[0]])*float(u2[p[1]]))/\n",
    "                    (float(u1.N())*float(u2.N()))),\n",
    "                   2)\n",
    "    except:\n",
    "        return\n",
    "\n",
    "fmi = {}\n",
    "for p in f.keys():\n",
    "    fmi[p]  = mutInf(p,u,u,f)\n",
    "\n",
    "fmi = {key: value for key, value in fmi.items() if value > 3}\n",
    "\n",
    "dict(sorted(fmi.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jklbichami\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus.reader.bnc import BNCCorpusReader\n",
    "from nltk import FreqDist, bigrams\n",
    "from itertools import chain\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.collocations import *\n",
    "nltk.download('stopwords')\n",
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "\n",
    "\n",
    "corpus = []\n",
    "\n",
    "zip_file_path = 'C:/Users/jklbichami/OneDrive - Valmet/Documents/School/Porgramming/NLP/2554.zip'\n",
    "\n",
    "bnc_reader = BNCCorpusReader(root=zip_file_path, fileids=r'[A-K]//w*//w*/.xml')\n",
    "\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    # List the files and folders in the zip archive\n",
    "    file_list = zip_ref.namelist()\n",
    "\n",
    "    for item in file_list:\n",
    "        if 'download/Texts/' in item and item.endswith('.xml'):\n",
    "            folder_name = item.rstrip('/')  # Extract folder name\n",
    "            corpus.append(bnc_reader.sents(folder_name))\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "words_to_include = ['woman', 'use', 'dream', 'body']\n",
    "\n",
    "# Get the words from the file in list format\n",
    "es = stopwords.words('english')\n",
    "\n",
    "\n",
    "# Mikhail This part took 41 mins to load on my machine\n",
    "g = FreqDist(bigrams(w.lower() for doc in corpus for sent in doc for w in sent if (w.isalpha() and w.lower() not in es)))\n",
    "\n",
    "f = FreqDist()\n",
    "for k in g.keys():\n",
    "    if k[1] in words_to_include or k[0] in words_to_include:\n",
    "        f[k] = g.get(k)\n",
    "\n",
    "## Here we probably should not limit words by anything, except just removing stop words\n",
    "u = FreqDist(w.lower() for w in corpus if (w.isalpha() and not(w.lower() in es)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({('per', 'cent'): 38049, ('gon', 'na'): 12436, ('last', 'year'): 10421, ('years', 'ago'): 10205, ('prime', 'minister'): 9467, ('last', 'night'): 8482, ('first', 'time'): 8379, ('would', 'like'): 8269, ('two', 'years'): 7427, ('united', 'states'): 7086, ...})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = FreqDist()\n",
    "for k in g.keys():\n",
    "    if k[1] in words_to_include or k[0] in words_to_include:\n",
    "        f[k] = g.get(k)\n",
    "\n",
    "# Here we probably should not limit words by anything, except just removing stop words\n",
    "u = FreqDist(w.lower() for w in chain.from_iterable(chain.from_iterable(corpus)) if (w.isalpha() and not(w.lower() in es)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o38.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (86H26M3.vstage.co executor driver): java.net.SocketException: Connection reset\r\n\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)\r\n\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)\r\n\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jklbichami\\OneDrive - Valmet\\Documents\\School\\Porgramming\\NLP\\Metaphor-detection-in-poetry\\Project NLP.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jklbichami/OneDrive%20-%20Valmet/Documents/School/Porgramming/NLP/Metaphor-detection-in-poetry/Project%20NLP.ipynb#Y113sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Create a DataFrame\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jklbichami/OneDrive%20-%20Valmet/Documents/School/Porgramming/NLP/Metaphor-detection-in-poetry/Project%20NLP.ipynb#Y113sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mcreateDataFrame(string_list, StringType())\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/jklbichami/OneDrive%20-%20Valmet/Documents/School/Porgramming/NLP/Metaphor-detection-in-poetry/Project%20NLP.ipynb#Y113sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m df\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\jklbichami\\Anaconda3\\envs\\nlp_course\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:959\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    953\u001b[0m     \u001b[39mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    954\u001b[0m         error_class\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNOT_BOOL\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    955\u001b[0m         message_parameters\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39marg_name\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mvertical\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39marg_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtype\u001b[39m(vertical)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[0;32m    956\u001b[0m     )\n\u001b[0;32m    958\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(truncate, \u001b[39mbool\u001b[39m) \u001b[39mand\u001b[39;00m truncate:\n\u001b[1;32m--> 959\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jdf\u001b[39m.\u001b[39mshowString(n, \u001b[39m20\u001b[39m, vertical))\n\u001b[0;32m    960\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    961\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jklbichami\\Anaconda3\\envs\\nlp_course\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\jklbichami\\Anaconda3\\envs\\nlp_course\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[39mexcept\u001b[39;00m Py4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\Users\\jklbichami\\Anaconda3\\envs\\nlp_course\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o38.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (86H26M3.vstage.co executor driver): java.net.SocketException: Connection reset\r\n\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)\r\n\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:829)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:186)\r\n\tat java.base/java.net.SocketInputStream.read(SocketInputStream.java:140)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:252)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:271)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:392)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"example\").getOrCreate()\n",
    "\n",
    "# Your list of strings\n",
    "string_list = [\"string1\", \"string2\", \"string3\"]\n",
    "\n",
    "# Create a DataFrame\n",
    "df = spark.createDataFrame(string_list, StringType())\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BNC full Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expression (2) in Neuman et al.’2013 paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See expression (2) in Neuman et al.’2013 paper\n",
    "import math\n",
    "def Mutual_information(bigram_item, filteredCorpus, Corpus, span = 2):    \n",
    "    '''\n",
    "    Calculates Mutual information between node and collocate words\n",
    "    bigram_item = The bigram which is considered in the equation\n",
    "    filteredCorpus = The corpus with only considered words.\n",
    "    Corpus = The whole corpus.\n",
    "    span = span of words\n",
    "    '''\n",
    "\n",
    "    # filteredCorpus[fr_B_near_A] = frequency of collocate near the node word (e.g., color near purple)\n",
    "    # Corpus.N() = size of the corpus (for instance 96,263,399, BNC)\n",
    "    # Corpus[p[0]] = frequency of node word w1 (e.g., purple): 1262\n",
    "    # Corpus[p[1]] = frequency of collocate word w2 (e.g., color): 115\n",
    "    # span = span of words (e.g., 1 to left and 1 to right of the node word: 2)\n",
    "\n",
    "    #sizeCorpus = 96 132 981 tokens in BNC\n",
    "\n",
    "    try:\n",
    "        return math.log10((filteredCorpus[bigram_item] * Corpus.N()) / (Corpus[p[0]] * Corpus[p[1]] * span))/math.log10(2)\n",
    "    except:\n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequencies calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = bnc_reader.words('A1E.xml')\n",
    "corpus = bnc_reader.words('A/A0/A01.xml')\n",
    "\n",
    "# Consider the words “woman”, “use”, “dream”, “body”.\n",
    "words_to_include = ['woman', 'use', 'dream', 'body']\n",
    "\n",
    "# Calculate the frequency of bigrams of all words in the corpus\n",
    "bigram_frequency = FreqDist(bigrams(w.lower() for w in corpus if (w.isalpha() and w.lower() not in stop_words)))\n",
    "\n",
    "# Calculate the frequency of bigrams of the words to include that are in the corpus\n",
    "fr_words_to_include = FreqDist()\n",
    "for key_word in bigram_frequency.keys():\n",
    "    # If first or second word in bigram, e.g. ('woman', 'receiving'), has a word_to_include\n",
    "    # Pass the frequency of this bigram from bigram_frequency to fr_words_to_include\n",
    "    if key_word[1] in words_to_include or key_word[0] in words_to_include:\n",
    "        fr_words_to_include[key_word] = bigram_frequency.get(key_word)\n",
    "\n",
    "# Frequency of all words in the corpus, except stop words\n",
    "unigram_frequency = FreqDist(w.lower() for w in corpus if (w.isalpha() and not(w.lower() in stop_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Mutual information frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('woman', 'receiving'): 9.925183519434208,\n",
       " ('illness', 'woman'): 8.925183519434208,\n",
       " ('multiplies', 'body'): 8.603255424546846,\n",
       " ('body', 'seriously'): 8.603255424546846,\n",
       " ('damage', 'body'): 8.603255424546846,\n",
       " ('body', 'kill'): 8.603255424546846,\n",
       " ('enters', 'body'): 7.603255424546846,\n",
       " ('body', 'enough'): 7.603255424546846,\n",
       " ('weakened', 'body'): 7.603255424546846,\n",
       " ('words', 'use'): 6.925183519434207,\n",
       " ('cells', 'body'): 6.6032554245468456,\n",
       " ('body', 'much'): 6.6032554245468456,\n",
       " ('use', 'condom'): 6.340221018713052,\n",
       " ('use', 'legacy'): 6.340221018713052,\n",
       " ('always', 'use'): 5.340221018713052,\n",
       " ('value', 'use'): 5.340221018713052,\n",
       " ('body', 'get'): 5.143823805909549,\n",
       " ('leaflet', 'use'): 4.925183519434208,\n",
       " ('could', 'use'): 4.2247438012931156,\n",
       " ('use', 'drugs'): 3.7552585179918956,\n",
       " ('use', 'deed'): 3.603255424546846,\n",
       " ('income', 'use'): 3.401621563377195,\n",
       " ('payment', 'use'): 3.0672025243066363,\n",
       " ('people', 'use'): 3.0182929238256895}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## If we have double nested list then we can use this to flatten it\n",
    "chain.from_iterable(corpus)\n",
    "\n",
    "# Identify all adjectives, adverbs and verbs that occur within 2 lexical units (span = 2 in the formula of mutual information) in BNC corpus\n",
    "# Mutual information frequency\n",
    "fmi = {}\n",
    "for p in fr_words_to_include.keys():\n",
    "    fmi[p]  = Mutual_information(p, fr_words_to_include, unigram_frequency)\n",
    "\n",
    "# Mutual information is equal or greater than 3, considered as the minimum statistical significance\n",
    "fmi = {key: value for key, value in fmi.items() if value > 3}\n",
    "\n",
    "dict(sorted(fmi.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suggest appropriate adjustments (e.g., greater span), if no results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suggest appropriate adjustments (e.g., greater span), if no results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to test this process in the previous metaphor annotated dataset. For this purpose, consider the following approach. Write a program that inputs each sentence of the annotated corpus, and then reads the head word (given in the annotation), then calculate the mutual distance between the head-word and each of the first two words occurring either on the left hand side part or right hand side part of the head-word. If the average of mutual distances from head word to each of the two words situated at two lexical units is greater than 3, then we shall consider the sentence is not a metaphor, otherwise, it is a metaphor. Test this reasoning and report the result for each annotated sentence and save it in your database. Given the ground truth of the annotated dataset, calculate the corresponding accuracy, and comment on the efficiency of the proposed approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Number</th>\n",
       "      <th>Head-word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>poise is a club .</td>\n",
       "      <td>y</td>\n",
       "      <td>1</td>\n",
       "      <td>poise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>destroying alexandria . sunlight is silence</td>\n",
       "      <td>y</td>\n",
       "      <td>4</td>\n",
       "      <td>sunlight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>feet are no anchor . gravity sucks at the mind</td>\n",
       "      <td>y</td>\n",
       "      <td>1</td>\n",
       "      <td>feet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>on the day 's horizon is a gesture of earth</td>\n",
       "      <td>y</td>\n",
       "      <td>5</td>\n",
       "      <td>horizon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>he said good-by as if good-by is a number .</td>\n",
       "      <td>y</td>\n",
       "      <td>6</td>\n",
       "      <td>good-by</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614</th>\n",
       "      <td>as the season of cold is the season of darkness</td>\n",
       "      <td>n</td>\n",
       "      <td>5</td>\n",
       "      <td>cold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615</th>\n",
       "      <td>else all beasts were tigers ,</td>\n",
       "      <td>y</td>\n",
       "      <td>3</td>\n",
       "      <td>beasts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>616</th>\n",
       "      <td>without which earth is sand</td>\n",
       "      <td>n</td>\n",
       "      <td>3</td>\n",
       "      <td>earth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>617</th>\n",
       "      <td>the sky is cloud on cloud</td>\n",
       "      <td>n</td>\n",
       "      <td>2</td>\n",
       "      <td>sky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>the sky is cloudy</td>\n",
       "      <td>n</td>\n",
       "      <td>2</td>\n",
       "      <td>sky</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>619 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Text Symbol  Number Head-word\n",
       "0                                  poise is a club .       y       1     poise\n",
       "1        destroying alexandria . sunlight is silence       y       4  sunlight\n",
       "2     feet are no anchor . gravity sucks at the mind       y       1      feet\n",
       "3        on the day 's horizon is a gesture of earth       y       5   horizon\n",
       "4        he said good-by as if good-by is a number .       y       6   good-by\n",
       "..                                                ...    ...     ...       ...\n",
       "614  as the season of cold is the season of darkness       n       5      cold\n",
       "615                    else all beasts were tigers ,       y       3    beasts\n",
       "616                      without which earth is sand       n       3     earth\n",
       "617                        the sky is cloud on cloud       n       2       sky\n",
       "618                                the sky is cloudy       n       2       sky\n",
       "\n",
       "[619 rows x 4 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://www.eecs.uottawa.ca/~diana/resources/metaphor/type1_metaphor_annotated.txt\"\n",
    "\n",
    "# Read the data into a DataFrame\n",
    "df = pd.read_csv(url, delimiter='\\t', header=None, names=['Text'])\n",
    "\n",
    "# Extract the symbol ('y' or 'n') and number into separate columns\n",
    "df['Symbol'] = df['Text'].str.extract(r'@(\\d+)@([yn])')[1]\n",
    "df['Number'] = df['Text'].str.extract(r'@(\\d+)@([yn])')[0]\n",
    "\n",
    "# Delete it from the original text\n",
    "df['Text'] = df['Text'].str.replace(r'(@\\d+@y|@\\d+@n)', '', regex=True)\n",
    "\n",
    "# Replace NaN values in 'Number' with 0, and then convert to integer\n",
    "df['Number'] = df['Number'].fillna(0).astype(int)\n",
    "\n",
    "# Extract the word with the specified index and save it in the 'Head-word' column\n",
    "df['Head-word'] = df.apply(lambda row: row['Text'].split()[int(row['Number']) - 1], axis=1)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input each sentence of the annotated corpus\n",
    "\n",
    "# Read the head word (given in the annotation)\n",
    "\n",
    "# Calculate the mutual distance between the head-word and each of the first two words\n",
    "# either on the left hand side part or right hand side part of the head-word.\n",
    "\n",
    "# If average is greater than 3, sentence is not a metaphor\n",
    "\n",
    "# Report the result for each annotated sentence \n",
    "\n",
    "# Save it in your database\n",
    "\n",
    "# Calculate the corresponding accuracy\n",
    "\n",
    "# Comment on the efficiency of the proposed approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider the (adjective-noun) type of metaphor (referred to as Metaphor type III). A metaphor  assumes to occur when the categories of noun and adjective are such that one is concrete and the other one is abstract. WordStat noun categorization based on WordNet, which classifies 69,817 nouns into 25 categories, of which 13 are concrete categories (e.g., artifact) provides a database for a such categorization. It is freely available in [`Wordnet based categorization dictionary - Provalis Research`](https://provalisresearch.com/products/content-analysis-software/wordstat-dictionary/wordnet-based-categorization-dictionary/). Write a program that allows you to retrieve the category of noun and adjective / adverb in a sentence according to WordStat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjective-noun type of metaphor is a Metaphor type III.\n",
    "\n",
    "# If categories of noun and adjective: one is concrete and the other one is abstract,\n",
    "# Then it is a Metaphor.\n",
    "\n",
    "# WordStat noun categorization based on WordNet provides a database for a such categorization.\n",
    "\n",
    "# Retrieve the category of noun and adjective / adverb in a sentence according to WordStat.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus.reader.bnc import BNCCorpusReader\n",
    "from nltk import FreqDist, bigrams\n",
    "import re\n",
    "\n",
    "file_p = r'C:\\Users\\jklbichami\\OneDrive - Valmet\\Documents\\School\\Porgramming\\NLP\\WordNet2\\WordNet Words & Phrases.CAT'\n",
    "\n",
    "with open(file_p, 'r') as file:\n",
    "    cat_data = file.readlines()\n",
    "\n",
    "pattern = r'\\.(.+)'\n",
    "\n",
    "topics = [a.strip('\\n') for a in cat_data if '\\t' not in a ]\n",
    "topics_indices = [cat_data.index(a + '\\n') for a in topics]\n",
    "\n",
    "noun_categories = [re.search(pattern, a).group(1).strip('\\n') for a in cat_data if '\\t\\t' not in a and a.startswith('\\tNOUN.')]\n",
    "noun_categories_idices = [cat_data.index('\\tNOUN.' + a + '\\n') for a in noun_categories]\n",
    "\n",
    "verb_categories = [re.search(pattern, a).group(1).strip('\\n') for a in cat_data if '\\t\\t' not in a and a.startswith('\\tVERB.')]\n",
    "verb_categories_idices = [cat_data.index('\\tVERB.' + a + '\\n') for a in verb_categories]\n",
    "\n",
    "adj_categories = [re.search(pattern, a).group(1).strip('\\n') for a in cat_data if '\\t\\t' not in a and a.startswith('\\tADJ.')]\n",
    "adj_categories_idices = [cat_data.index('\\tADJ.' + a + '\\n') for a in adj_categories]\n",
    "\n",
    "cat_df = pd.DataFrame(columns=['Type', 'Category', 'Word'])\n",
    "\n",
    "\n",
    "## Mikhail: did not finish this but idea is pretty simple\n",
    "## 1. We get the indeces of all topics\n",
    "## 2. We get indeces for respective categories\n",
    "## 3. We iterate through each topics\n",
    "## 4. We iterate through their categories\n",
    "## 5. Assign all results to df based on indices\n",
    "## Here is examplee for Nouns\n",
    "for indx, num in enumerate(noun_categories_idices):\n",
    "    temp_dict = {}\n",
    "    if indx == 0:\n",
    "        temp_dict['Category'] = [noun_categories[indx]]*abs(cat_data.index('NOUNS\\n')+1-noun_categories_idices[indx+1]+1)\n",
    "        temp_dict['Type'] = ['Noun']*abs(cat_data.index('NOUNS\\n')+1-noun_categories_idices[indx+1]+1)\n",
    "        temp_dict['Word'] = [a[2:].strip('(1)\\n)').lower() for a in  cat_data[cat_data.index('NOUNS\\n')+2:noun_categories_idices[indx+1]]]\n",
    "        cat_df = pd.concat([cat_df, pd.DataFrame(temp_dict)], ignore_index=True)\n",
    "    else:\n",
    "        try:\n",
    "            temp_dict['Category'] = [noun_categories[indx]]*abs(noun_categories_idices[indx]+1-noun_categories_idices[indx+1])\n",
    "            temp_dict['Type'] = ['Noun']*abs(noun_categories_idices[indx]+1-noun_categories_idices[indx+1])\n",
    "            temp_dict['Word'] = [a[2:].strip('(1)\\n)').lower() for a in  cat_data[noun_categories_idices[indx]+1:noun_categories_idices[indx+1]]]\n",
    "            cat_df = pd.concat([cat_df, pd.DataFrame(temp_dict)], ignore_index=True)\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "for indx, num in enumerate(verb_categories):\n",
    "    temp_dict = {}\n",
    "    if indx == 0:\n",
    "        temp_dict['Category'] = [verb_categories[indx]]*abs(cat_data.index('VERBS\\n')+1-verb_categories_idices[indx+1]+1)\n",
    "        temp_dict['Type'] = ['Verb']*abs(cat_data.index('VERBS\\n')+1-verb_categories_idices[indx+1]+1)\n",
    "        temp_dict['Word'] = [a[2:].strip('(1)\\n)').lower() for a in  cat_data[cat_data.index('VERBS\\n')+2:verb_categories_idices[indx+1]]]\n",
    "        cat_df = pd.concat([cat_df, pd.DataFrame(temp_dict)], ignore_index=True)\n",
    "    else:\n",
    "        try:\n",
    "            temp_dict['Category'] = [verb_categories[indx]]*abs(verb_categories_idices[indx]+1-verb_categories_idices[indx+1])\n",
    "            temp_dict['Type'] = ['Verb']*abs(verb_categories_idices[indx]+1-verb_categories_idices[indx+1])\n",
    "            temp_dict['Word'] = [a[2:].strip('(1)\\n)').lower() for a in  cat_data[verb_categories_idices[indx]+1:verb_categories_idices[indx+1]]]\n",
    "            cat_df = pd.concat([cat_df, pd.DataFrame(temp_dict)], ignore_index=True)\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "for indx, num in enumerate(adj_categories):\n",
    "    temp_dict = {}\n",
    "    if indx == 0:\n",
    "        temp_dict['Category'] = [adj_categories[indx]]*abs(cat_data.index('ADJECTIVES\\n')+1-adj_categories_idices[indx+1]+1)\n",
    "        temp_dict['Type'] = ['Adjective']*abs(cat_data.index('ADJECTIVES\\n')+1-adj_categories_idices[indx+1]+1)\n",
    "        temp_dict['Word'] = [a[2:].strip('(1)\\n)').lower() for a in  cat_data[cat_data.index('ADJECTIVES\\n')+2:adj_categories_idices[indx+1]]]\n",
    "        cat_df = pd.concat([cat_df, pd.DataFrame(temp_dict)], ignore_index=True)\n",
    "    else:\n",
    "        try:\n",
    "            temp_dict['Category'] = [adj_categories[indx]]*abs(adj_categories_idices[indx]+1-adj_categories_idices[indx+1])\n",
    "            temp_dict['Type'] = ['Adjective']*abs(adj_categories_idices[indx]+1-adj_categories_idices[indx+1])\n",
    "            temp_dict['Word'] = [a[2:].strip('(1)\\n)').lower() for a in  cat_data[adj_categories_idices[indx]+1:adj_categories_idices[indx+1]]]\n",
    "            cat_df = pd.concat([cat_df, pd.DataFrame(temp_dict)], ignore_index=True)\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "temp_dict = {}\n",
    "temp_dict['Category'] = [adj_categories[indx]]*abs(cat_data.index('ADVERBS\\n')+2-cat_data.index('NOUNS\\n'))\n",
    "temp_dict['Type'] = ['Adjective']*abs(cat_data.index('ADVERBS\\n')+2-cat_data.index('NOUNS\\n'))\n",
    "temp_dict['Word'] = [a[2:].strip('(1)\\n)').lower() for a in  cat_data[cat_data.index('ADVERBS\\n')+2:cat_data.index('NOUNS\\n')]]\n",
    "cat_df = pd.concat([cat_df, pd.DataFrame(temp_dict)], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Category</th>\n",
       "      <th>Word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>150326</th>\n",
       "      <td>Adjective</td>\n",
       "      <td>ALL</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150327</th>\n",
       "      <td>Adjective</td>\n",
       "      <td>ALL</td>\n",
       "      <td>a.d.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150328</th>\n",
       "      <td>Adjective</td>\n",
       "      <td>ALL</td>\n",
       "      <td>a.k.a.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150329</th>\n",
       "      <td>Adjective</td>\n",
       "      <td>ALL</td>\n",
       "      <td>a.m.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150330</th>\n",
       "      <td>Adjective</td>\n",
       "      <td>ALL</td>\n",
       "      <td>a_bit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154988</th>\n",
       "      <td>Adjective</td>\n",
       "      <td>ALL</td>\n",
       "      <td>youthfully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154989</th>\n",
       "      <td>Adjective</td>\n",
       "      <td>ALL</td>\n",
       "      <td>zealously</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154990</th>\n",
       "      <td>Adjective</td>\n",
       "      <td>ALL</td>\n",
       "      <td>zestfully</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154991</th>\n",
       "      <td>Adjective</td>\n",
       "      <td>ALL</td>\n",
       "      <td>zestily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154992</th>\n",
       "      <td>Adjective</td>\n",
       "      <td>ALL</td>\n",
       "      <td>zigzag</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4667 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Type Category         Word\n",
       "150326  Adjective      ALL           a \n",
       "150327  Adjective      ALL        a.d. \n",
       "150328  Adjective      ALL      a.k.a. \n",
       "150329  Adjective      ALL        a.m. \n",
       "150330  Adjective      ALL       a_bit \n",
       "...           ...      ...          ...\n",
       "154988  Adjective      ALL  youthfully \n",
       "154989  Adjective      ALL   zealously \n",
       "154990  Adjective      ALL   zestfully \n",
       "154991  Adjective      ALL     zestily \n",
       "154992  Adjective      ALL      zigzag \n",
       "\n",
       "[4667 rows x 3 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_df[cat_df['Category'] == 'ALL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would like to imitate the procedure mentioned in Neuman’s paper for type III semaphore. Write a program that identifies the occurrence of Noun-Adjective/Adverb part-of-speech in a given sentence. Then, use WordNet lexical database to find out the number of senses of each adjective. If every adjective has one single sense, then return, no metaphor. If the Noun has no entry in wordnet, then return UNKNOWN. Otherwise (adjective has more than one sense and noun has an entry in WordNet), then identify the set S of nouns in the BNC corpus that collocate with the given Noun of the given sentence (this corresponds to a set of nouns whose mutual information value is greater or equal than 3). Next, for each element (noun) of S, use the WordStat categorization to identify those who belong to concrete class. Let S1 be a subset of S, which contains these “concrete”-category nouns. If the number of elements in S1 is large, then restrict to the first three elements who have the highest mutual information values. Finally, to find out whether, whether the sentence containing adjective A and noun N is a metaphor, we need to test the compatibility of each elements of S1 with N. If there is no elements in S1 compatible with N, then we shall consider S as a metaphor, otherwise, it is not. To evaluate this compatibility, you can use the Wu and Palmer WordNet semantic similarity already implemented in NLTK. Therefore, assume that if the Wu and Palmer semantic similarity of at least of the nouns in S1 with N is greater than a threshold 0.4, then the compatibility between S1 and N is granted. (Note this is only a very rough approximation). Write a code that implements this reasoning and test it on two simple examples of your choice. Test this process for other values of threshold values (e.g., 0.3, 0.5, 0.6) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# Imitate the procedure mentioned in Neuman’s paper for type III semaphore\n",
    "\n",
    "# Identify the occurrence of Noun-Adjective/Adverb part-of-speech in a given sentence.\n",
    "\n",
    "# Use WordNet lexical database to find out the number of senses of each adjective.\n",
    "\n",
    "# If every adjective has one single sense, then no metaphor\n",
    "\n",
    "# If the Noun has no entry in wordnet, then return UNKNOWN.\n",
    "\n",
    "# Otherwise: adjective has more than one sense and noun has an entry in WordNet\n",
    "\n",
    "# Identify the set S of nouns in the BNC corpus that collocate with the given Noun of the given sentence\n",
    "# set of nouns: mutual information value is >= 3\n",
    "\n",
    "# Use the WordStat categorization for each noun in S to identify those who belong to concrete class.\n",
    "\n",
    "# Let S1 be a subset of S, which contains these “concrete”-category nouns.\n",
    "# If the number of elements in S1 is large, then restrict to the first three elements who have the highest mutual information values.\n",
    "\n",
    "# Test the compatibility of each elements of S1 with N.\n",
    "# To check if the sentence containing adjective A and noun N is a metaphor\n",
    "\n",
    "# If there is no elements in S1 compatible with N, then we shall consider S as a metaphor\n",
    "\n",
    "# Evaluate, using the Wu and Palmer WordNet semantic similarity from NLTK.\n",
    "\n",
    "# if semantic similarity of at least of the nouns in S1 with N >= 0.4, then the compatibility between S1 and N is granted.\n",
    "\n",
    "# Test it on two simple examples of your choice. \n",
    "\n",
    "# Test this process for other values of threshold values (e.g., 0.3, 0.5, 0.6) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the above reasoning on the first half of the dataset https://www.eecs.uottawa.ca/~diana/resources/metaphor/type1_metaphor_annotated.txt where adjective-noun type of relationship occurs. Motivate your reasoning and answers. Estimate the accuracy accordingly, and report individual results in your database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the above reasoning on the first half of the dataset\n",
    "\n",
    "# Motivate your reasoning and answers.\n",
    "\n",
    "# Estimate the accuracy accordingly, and report individual results in your database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of the calculus of the semantic similarity between N and each elements of S1 in step 4, we would like to use the wordnet domain of each individual words. For this purpose, download the wordnet domain from [`WordNet Domains (fbk.eu)`](https://wndomains.fbk.eu/download.html). Therefore, the compatibility between N and an element N1 of S1 is granted if N and N1 belong to the same wordnet domain. Write a program that allows you to implement this reasoning and test it on simple sentences of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the wordnet domain of each individual words, instead of similarity between N and each elements of S1\n",
    "\n",
    "# Download the wordnet domain\n",
    "\n",
    "# If N and N1 belong to the same wordnet domain, compatibility between N and an element N1 of S1 is granted\n",
    "\n",
    "# Test it on simple sentences of your choice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the reasoning of 6) on the same subset of annotated metaphor dataset used in 5) and compare the performance in terms of accuracy. Save individual results in your database as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the reasoning of 6) on dataset used in 5)\n",
    "\n",
    "# Compare the performance in terms of accuracy\n",
    "\n",
    "# Save individual results in your database as well\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat 6) and 7) when using Reuter corpus (also accessible via NLTK) instead of BNC corpus. Conclude on the impact of the corpus on the accuracy of metaphor identification.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat 6) and 7) when using Reuter corpus from NLTK\n",
    "\n",
    "# Conclude on the impact of the corpus on the accuracy of metaphor identification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to enhance the reasoning of any of project specifications above, feel free to suggest any state-of-the-art approach that you judge relevant and accommodate to achieve the goal accordingly. Motivate your choice by concise literature review. Use appropriate literature to discuss your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhance the reasoning of any of project specifications above\n",
    "\n",
    "# Suggest any relevant state-of-the-art approach\n",
    "\n",
    "# Motivate your choice by concise literature review. \n",
    "\n",
    "# Use appropriate literature to discuss your findings.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
